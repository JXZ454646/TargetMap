{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Jul2024_release.txt', sep='\\t')\n",
    "# 筛选DATABASEA为UNIPROT或ChEBI的行\n",
    "filtered_df_A = df[(df['DATABASEA'] == 'UNIPROT') | (df['DATABASEA'] == 'ChEBI')]\n",
    "\n",
    "# 筛选DATABASEB为UNIPROT或ChEBI的行\n",
    "filtered_df_B = df[(df['DATABASEB'] == 'UNIPROT') | (df['DATABASEB'] == 'ChEBI')]\n",
    "\n",
    "# 创建字典\n",
    "entity_id_dict = {row['IDA']: row['ENTITYA'] for _, row in filtered_df_A.iterrows()}\n",
    "entity_id_dict.update({row['IDB']: row['ENTITYB'] for _, row in filtered_df_B.iterrows()})\n",
    "\n",
    "entity_id_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('entity_id_dict.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(entity_id_dict, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities.txt', 'w') as f:\n",
    "    for k, v in sorted_temp_map2.items():\n",
    "        f.write(f'{k}\\t{v}\\n')\n",
    "        \n",
    "with open('all.txt', 'w') as f:\n",
    "    for row in filtered_c.itertuples():\n",
    "        f.write(f'{row.Start}\\t{row.Relation}\\t{row.End}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_file = \"all.txt\"\n",
    "train_file = \"train.txt\"\n",
    "valid_file = \"valid.txt\"\n",
    "test_file = \"test.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "train_data, temp_data = train_test_split(lines, test_size=0.2, random_state=42)  # 80% train, 20% temp\n",
    "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 10% valid, 10% test\n",
    "\n",
    "with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_data)\n",
    "\n",
    "with open(valid_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(valid_data)\n",
    "\n",
    "with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
